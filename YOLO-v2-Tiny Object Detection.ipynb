{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bd7c22-3fb9-4831-aaa6-c860c872778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/personal/Day4/object_detection\")\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils.im_utils import Compose, ToTensor, RandomHorizontalFlip\n",
    "from utils.plot_utils import plot_loss_and_lr, plot_map\n",
    "from utils.train_utils import train_one_epoch, write_tb, create_model\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torch\n",
    "import math\n",
    "import pdb\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107d09fe-f63b-42f8-8ac4-8394df90656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "\n",
    "    # data transform parameter\n",
    "    train_horizon_flip_prob = 0.0  # data horizon flip probility in train transform\n",
    "    min_size = 800\n",
    "    max_size = 1000\n",
    "    image_mean = [0.485, 0.456, 0.406]\n",
    "    image_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    # anchor parameters\n",
    "    anchors = [(10,20),(10,10),(20,10),(32,64),(64,32),(64,64),(128,128)]\n",
    "    num_anchors = len(anchors)\n",
    "\n",
    "    # data parameters\n",
    "    num_classes = 4\n",
    "    image_size = 640\n",
    "\n",
    "    # loss parameters\n",
    "    noobject_scale = 1\n",
    "    object_scale = 1\n",
    "    coord_scale = 1\n",
    "    class_scale = 1\n",
    "\n",
    "    # iou parameters\n",
    "    thresh = 0.45\n",
    "\n",
    "    # training parameters\n",
    "    device_name = 'cuda'\n",
    "    start_epoch = 0  # start epoch\n",
    "    num_epochs = 30  # train epochs\n",
    "    lr = 1e-4\n",
    "    momentum = 0.9\n",
    "    weight_decay = 0.0005\n",
    "\n",
    "    # learning rate schedule\n",
    "    lr_gamma = 0.33\n",
    "    lr_dec_step_size = 100\n",
    "    batch_size = 4\n",
    "    \n",
    "    # dataset\n",
    "    train_anno_path = \"data/SkyFusion/train/_annotations.coco.json\"\n",
    "    train_image_dir = \"data/SkyFusion/train/\"\n",
    "    valid_anno_path = \"data/SkyFusion/valid/_annotations.coco.json\"\n",
    "    valid_image_dir = \"data/SkyFusion/valid/\"\n",
    "    test_anno_path = \"data/SkyFusion/test/_annotations.coco.json\"\n",
    "    test_image_dir = \"data/SkyFusion/test/\"\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d0ea6f-a4fe-42b4-8b4b-3297c050c7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class coco(torch.utils.data.Dataset):\n",
    "    def __init__(self, anno_path, image_dir, transforms=None):\n",
    "\n",
    "        self._anno_path = anno_path\n",
    "        self._image_dir = image_dir\n",
    "        self._transforms = transforms\n",
    "\n",
    "        with open(self._anno_path) as anno_file:\n",
    "            self.anno = json.load(anno_file)\n",
    "            \n",
    "        cats = self.anno['categories']\n",
    "        \n",
    "        cats = sorted(cats, key=lambda x: x['id'])\n",
    "        self._classes = tuple(['__background__'] + [c['name'] for c in cats])\n",
    "        \n",
    "        self.classes = self._classes\n",
    "        self.num_classes = len(self.classes)\n",
    "        self._class_to_ind = dict(list(zip(self.classes, list(range(self.num_classes)))))\n",
    "        self._class_to_coco_cat_id = dict(list(zip([c['name'] for c in cats], [c['id'] for c in cats])))\n",
    "        self.coco_cat_id_to_class_ind = dict([(self._class_to_coco_cat_id[cls], self._class_to_ind[cls]) for cls in self._classes[1:]])\n",
    "        \n",
    "        # print(cats)\n",
    "        # print(self._class_to_ind)\n",
    "        # print(self._class_to_coco_cat_id)\n",
    "        # print(self.coco_cat_id_to_class_ind)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anno['images'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        a = self.anno['images'][idx]\n",
    "        image_idx = a['id']\n",
    "        image_file_name = a['file_name']\n",
    "        img_path = os.path.join(self._image_dir, image_file_name)\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        width = a['width']\n",
    "        height = a['height']\n",
    "\n",
    "        iscrowd = None  # 与原始代码一致\n",
    "        annIds = []\n",
    "        objs = []\n",
    "        \n",
    "        # 遍历所有标注，筛选出属于当前图片的标注\n",
    "        for ann in self.anno['annotations']:\n",
    "            # 检查图片ID是否匹配\n",
    "            if ann['image_id'] == image_idx:\n",
    "                # 检查 iscrowd 条件（None 表示不限制）\n",
    "                if iscrowd is None or ann['iscrowd'] == iscrowd:\n",
    "                    annIds.append(ann['id'])\n",
    "                    objs.append(ann)\n",
    "\n",
    "        # Sanitize bboxes -- some are invalid\n",
    "        valid_objs = []\n",
    "        for obj in objs:\n",
    "            x1 = np.max((0, obj['bbox'][0]))\n",
    "            y1 = np.max((0, obj['bbox'][1]))\n",
    "            x2 = np.min((width - 1, x1 + np.max((0, obj['bbox'][2] - 1))))\n",
    "            y2 = np.min((height - 1, y1 + np.max((0, obj['bbox'][3] - 1))))\n",
    "            if obj['area'] > 0 and x2 > x1 and y2 > y1:\n",
    "                obj['clean_bbox'] = [x1, y1, x2, y2]\n",
    "                valid_objs.append(obj)\n",
    "        objs = valid_objs\n",
    "        num_objs = len(objs)\n",
    "\n",
    "        boxes = np.zeros((num_objs, 4), dtype=np.float32)\n",
    "        gt_classes = np.zeros((num_objs), dtype=np.int32)\n",
    "\n",
    "        iscrowd = []\n",
    "        for ix, obj in enumerate(objs):\n",
    "            cls = self.coco_cat_id_to_class_ind[obj['category_id']]\n",
    "            boxes[ix, :] = obj['clean_bbox']\n",
    "            gt_classes[ix] = cls\n",
    "            iscrowd.append(int(obj[\"iscrowd\"]))\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        image_id = torch.tensor([image_idx])\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        gt_classes = torch.as_tensor(gt_classes, dtype=torch.int32)\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int32)\n",
    "\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        target = {\"boxes\": boxes.float(), \"labels\": gt_classes.float(), \"image_id\": image_id.long(), \"area\": area.float(), \"iscrowd\": iscrowd.float(), \"num_objs\": torch.tensor([num_objs],dtype=torch.int32)}\n",
    "\n",
    "        if self._transforms is not None:\n",
    "            image, target = self._transforms(image, target)\n",
    "\n",
    "        padding_rows = 150 - target[\"boxes\"].size(0)\n",
    "        if padding_rows > 0:\n",
    "            zeros = torch.zeros(padding_rows,4).float()\n",
    "            target[\"boxes\"] = torch.cat([target[\"boxes\"],zeros],dim=0).to(torch.float32)\n",
    "            zeros = torch.zeros(padding_rows).float()\n",
    "            for k in [\"labels\",\"area\",\"iscrowd\"]:\n",
    "                target[k] = torch.cat([target[k],zeros],dim=0).to(torch.float32)\n",
    "        elif padding_rows < 0:\n",
    "            for k in [\"boxes\",\"labels\",\"area\",\"iscrowd\"]:\n",
    "                target[k] = target[k][:150].to(torch.float32)\n",
    "            target[\"num_objs\"] = torch.tensor([150],dtype=torch.int32)\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        return tuple(zip(*batch))\n",
    "\n",
    "    @property\n",
    "    def class_to_coco_cat_id(self):\n",
    "        return self._class_to_coco_cat_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2109d2ed-09d7-4e39-861f-2444e2241d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(cfg.device_name)\n",
    "print(\"Using {} device training.\".format(device.type))\n",
    "\n",
    "data_transform = {\n",
    "    \"train\": Compose([ToTensor(), RandomHorizontalFlip(cfg.train_horizon_flip_prob)]),\n",
    "    \"val\": Compose([ToTensor()])\n",
    "}\n",
    "\n",
    "# load train data set\n",
    "train_data_set = coco(cfg.train_anno_path, cfg.train_image_dir, data_transform[\"train\"])\n",
    "batch_size = cfg.batch_size\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data_set,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=True,\n",
    "                                                num_workers=8)\n",
    "\n",
    "# load validation data set\n",
    "val_data_set = coco(cfg.valid_anno_path, cfg.valid_image_dir, data_transform[\"val\"])\n",
    "val_data_set_loader = torch.utils.data.DataLoader(val_data_set,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False,\n",
    "                                                  num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa19ac1-f067-4baa-9c66-e557d0786ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### passthrough部分的处理\n",
    "def reorg(x, stride_h=2, stride_w=2):\n",
    "    batch_size, channels, height, width = x.size()\n",
    "    _height, _width = height // stride_h, width // stride_w\n",
    "    if 1:\n",
    "        x = x.view(batch_size, channels, _height, stride_h, _width, stride_w).transpose(3, 4).contiguous()\n",
    "        x = x.view(batch_size, channels, _height * _width, stride_h * stride_w).transpose(2, 3).contiguous()\n",
    "        x = x.view(batch_size, channels, stride_h * stride_w, _height, _width).transpose(1, 2).contiguous()\n",
    "        x = x.view(batch_size, -1, _height, _width)\n",
    "    else:\n",
    "        x = x.view(batch_size, channels, _height, stride_h, _width, stride_w)\n",
    "        x = x.permute(0, 1, 3, 5, 2, 4) # batch_size, channels, stride, stride, _height, _width\n",
    "        x = x.contiguous()\n",
    "        x = x.view(batch_size, -1, _height, _width)\n",
    "    return x\n",
    "\n",
    "\n",
    "### 卷积块\n",
    "class Conv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=0, stride=1):\n",
    "        nn.Module.__init__(self)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding='same')\n",
    "        self.bn = nn.BatchNorm2d(out_channels, momentum=0.01)\n",
    "        self.act = nn.LeakyReLU(0.1, inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "### 模型\n",
    "class YoloV2(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels=3,\n",
    "                 anchors=cfg.anchors,\n",
    "                 num_classes=cfg.num_classes,\n",
    "                 filter_thres = 0.5,\n",
    "                 nms_thres = 0.45,\n",
    "                ):\n",
    "        nn.Module.__init__(self)\n",
    "        \"\"\"\n",
    "        model output：H * W * ( num_anchors * ( 5 + num_classes ) )\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.anchors = anchors\n",
    "        self.filter_thres = filter_thres\n",
    "        self.nms_thres = nms_thres\n",
    "        self.out_channels = len(anchors) * (5 + num_classes)\n",
    "        \n",
    "        layers = []\n",
    "\n",
    "        # two 3*3 convolution layers\n",
    "        layers.append(Conv2d(in_channels, 32, kernel_size = 3))\n",
    "        layers.append(nn.MaxPool2d(kernel_size = 2))\n",
    "        \n",
    "        layers.append(Conv2d(32, 64, kernel_size = 3))\n",
    "        layers.append(nn.MaxPool2d(kernel_size = 2))\n",
    "\n",
    "        # two 3*3+1*1+3*3 convolution blocks\n",
    "        layers.append(Conv2d(64, 128, kernel_size = 3))\n",
    "        layers.append(Conv2d(128, 64, kernel_size = 1))\n",
    "        layers.append(Conv2d(64, 128, kernel_size = 3))\n",
    "        layers.append(nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        layers.append(Conv2d(128, 256, kernel_size = 3))\n",
    "        layers.append(Conv2d(256, 128, kernel_size = 1))\n",
    "        layers.append(Conv2d(128, 256, kernel_size = 3))\n",
    "        self.layers1 = nn.Sequential(*layers) # 实例化\n",
    "\n",
    "        # 在此处把输入拿走进行passthrough\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # 升到512维度之后，先pssthrough，再MaxPool\n",
    "        layers.append(nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "        # some large 3*3 and 1*1 combo\n",
    "        layers.append(Conv2d(256, 512, kernel_size = 3))\n",
    "        layers.append(Conv2d(512, 256, kernel_size = 1))\n",
    "        layers.append(Conv2d(256, 512, kernel_size = 3))\n",
    "        layers.append(Conv2d(512, 256, kernel_size = 1))\n",
    "        layers.append(Conv2d(256, 512, kernel_size = 3))\n",
    "        self.layers2 = nn.Sequential(*layers)\n",
    "\n",
    "        # 在这里吧passthrough的结果拿来concat\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "        # 更多的 3*3 + 1*1\n",
    "        layers.append(Conv2d(1536, 1024, kernel_size = 3))\n",
    "        layers.append(Conv2d(1024, 512, kernel_size = 1))\n",
    "        layers.append(Conv2d(512, 1024, kernel_size = 3))\n",
    "        layers.append(Conv2d(1024, 512, kernel_size = 1))\n",
    "        layers.append(Conv2d(512, 1024, kernel_size = 3))\n",
    "        layers.append(Conv2d(1024, self.out_channels, kernel_size = 1))\n",
    "        self.layers3 = nn.Sequential(*layers)\n",
    "\n",
    "        self.init()\n",
    "\n",
    "    def init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                m.weight = nn.init.kaiming_normal(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers1(x)\n",
    "        _x = reorg(x)\n",
    "        x = self.layers2(x)\n",
    "        x = torch.cat([_x, x], 1)\n",
    "        x = self.layers3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1748ade7-d01b-4731-90da-57a7c2427acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_ious(box1, box2): ### input B * (x1, y1, x2, y2)\n",
    "    \"\"\"\n",
    "    Implement the intersection over union (IoU) between box1 and box2 (x1, y1, x2, y2)\n",
    "\n",
    "    Arguments:\n",
    "    box1 -- tensor of shape (N, 4), first set of boxes\n",
    "    box2 -- tensor of shape (K, 4), second set of boxes\n",
    "\n",
    "    Returns:\n",
    "    ious -- tensor of shape (N, K), ious between boxes\n",
    "    \"\"\"\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "\n",
    "        N = box1.size(0)\n",
    "        K = box2.size(0)\n",
    "    \n",
    "        # 创建显式副本\n",
    "        box1_x1 = box1[:, 0].clone().reshape(N, 1)\n",
    "        box2_x1 = box2[:, 0].clone().reshape(1, K)\n",
    "        xi1 = torch.max(box1_x1, box2_x1)\n",
    "\n",
    "        box1_y1 = box1[:, 1].clone().reshape(N, 1)\n",
    "        box2_y1 = box2[:, 1].clone().reshape(1, K)\n",
    "        yi1 = torch.max(box1_y1, box2_y1)\n",
    "\n",
    "        box1_x2 = box1[:, 2].clone().reshape(N, 1)\n",
    "        box2_x2 = box2[:, 2].clone().reshape(1, K)\n",
    "        xi2 = torch.max(box1_x2, box2_x2)\n",
    "\n",
    "        box1_y2 = box1[:, 3].clone().reshape(N, 1)\n",
    "        box2_y2 = box2[:, 3].clone().reshape(1, K)\n",
    "        yi2 = torch.max(box1_y2, box2_y2)\n",
    "        \n",
    "        iw = torch.max(xi2 - xi1, torch.tensor(0.0, device=box1.device))\n",
    "        ih = torch.max(yi2 - yi1, torch.tensor(0.0, device=box1.device))\n",
    "    \n",
    "        inter = iw * ih\n",
    "    \n",
    "        # 计算面积时使用非 inplace 操作\n",
    "        box1_area = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n",
    "        box2_area = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n",
    "    \n",
    "        # 使用 view 而不是 reshape\n",
    "        box1_area = box1_area.view(N, 1)\n",
    "        box2_area = box2_area.view(1, K)\n",
    "    \n",
    "        union_area = box1_area + box2_area - inter\n",
    "    \n",
    "        # 添加小量防止除以0（非 inplace 操作）\n",
    "        ious = inter / (union_area + 1e-10)\n",
    "    \n",
    "        return ious\n",
    "\n",
    "def xxyy2xywh(box):\n",
    "    \"\"\"\n",
    "    Convert the box (x1, y1, x2, y2) encoding format to (c_x, c_y, w, h) format\n",
    "\n",
    "    Arguments:\n",
    "    box: tensor of shape (N, 4), boxes of (x1, y1, x2, y2) format\n",
    "\n",
    "    Returns:\n",
    "    xywh_box: tensor of shape (N, 4), boxes of (c_x, c_y, w, h) format\n",
    "    \"\"\"\n",
    "\n",
    "    c_x = (box[:, 2] + box[:, 0]) / 2\n",
    "    c_y = (box[:, 3] + box[:, 1]) / 2\n",
    "    w = box[:, 2] - box[:, 0]\n",
    "    h = box[:, 3] - box[:, 1]\n",
    "\n",
    "    c_x = c_x.view(-1, 1)\n",
    "    c_y = c_y.view(-1, 1)\n",
    "    w = w.view(-1, 1)\n",
    "    h = h.view(-1, 1)\n",
    "\n",
    "    xywh_box = torch.cat([c_x, c_y, w, h], dim=1)\n",
    "    return xywh_box\n",
    "\n",
    "\n",
    "def xywh2xxyy(box):\n",
    "    \"\"\"\n",
    "    Convert the box encoding format form (c_x, c_y, w, h) to (x1, y1, x2, y2)\n",
    "\n",
    "    Arguments:\n",
    "    box -- tensor of shape (N, 4), box of (c_x, c_y, w, h) format\n",
    "\n",
    "    Returns:\n",
    "    xxyy_box -- tensor of shape (N, 4), box of (x1, y1, x2, y2) format\n",
    "    \"\"\"\n",
    "\n",
    "    x1 = box[:, 0] - (box[:, 2]) / 2\n",
    "    y1 = box[:, 1] - (box[:, 3]) / 2\n",
    "    x2 = box[:, 0] + (box[:, 2]) / 2\n",
    "    y2 = box[:, 1] + (box[:, 3]) / 2\n",
    "\n",
    "    x1 = x1.view(-1, 1)\n",
    "    y1 = y1.view(-1, 1)\n",
    "    x2 = x2.view(-1, 1)\n",
    "    y2 = y2.view(-1, 1)\n",
    "\n",
    "    xxyy_box = torch.cat([x1, y1, x2, y2], dim=1)\n",
    "    return xxyy_box\n",
    "\n",
    "\n",
    "def box_transform(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate the delta values σ(t_x), σ(t_y), exp(t_w), exp(t_h) used for transforming box1 to  box2\n",
    "\n",
    "    Arguments:\n",
    "    box1 -- tensor of shape (N, 4) first set of boxes (c_x, c_y, w, h)\n",
    "    box2 -- tensor of shape (N, 4) second set of boxes (c_x, c_y, w, h)\n",
    "\n",
    "    Returns:\n",
    "    deltas -- tensor of shape (N, 4) delta values (t_x, t_y, t_w, t_h)\n",
    "                   used for transforming boxes to reference boxes\n",
    "    \"\"\"\n",
    "\n",
    "    t_x = box2[:, 0] - box1[:, 0]\n",
    "    t_y = box2[:, 1] - box1[:, 1]\n",
    "    t_w = box2[:, 2] / box1[:, 2]\n",
    "    t_h = box2[:, 3] / box1[:, 3]\n",
    "\n",
    "    t_x = t_x.view(-1, 1)\n",
    "    t_y = t_y.view(-1, 1)\n",
    "    t_w = t_w.view(-1, 1)\n",
    "    t_h = t_h.view(-1, 1)\n",
    "\n",
    "    # σ(t_x), σ(t_y), exp(t_w), exp(t_h)\n",
    "    deltas = torch.cat([t_x, t_y, t_w, t_h], dim=1)\n",
    "    return deltas\n",
    "\n",
    "\n",
    "def box_transform_inv(box, deltas):\n",
    "    \"\"\"\n",
    "    apply deltas to box to generate predicted boxes\n",
    "\n",
    "    Arguments:\n",
    "    box -- tensor of shape (N, 4), boxes, (c_x, c_y, w, h)\n",
    "    deltas -- tensor of shape (N, 4), deltas, (σ(t_x), σ(t_y), exp(t_w), exp(t_h))\n",
    "\n",
    "    Returns:\n",
    "    pred_box -- tensor of shape (N, 4), predicted boxes, (c_x, c_y, w, h)\n",
    "    \"\"\"\n",
    "\n",
    "    c_x = box[:, 0] + deltas[:, 0]\n",
    "    c_y = box[:, 1] + deltas[:, 1]\n",
    "    w = box[:, 2] * deltas[:, 2]\n",
    "    h = box[:, 3] * deltas[:, 3]\n",
    "\n",
    "    c_x = c_x.view(-1, 1)\n",
    "    c_y = c_y.view(-1, 1)\n",
    "    w = w.view(-1, 1)\n",
    "    h = h.view(-1, 1)\n",
    "\n",
    "    pred_box = torch.cat([c_x, c_y, w, h], dim=-1)\n",
    "    return pred_box\n",
    "\n",
    "\n",
    "def generate_all_anchors(anchors, H, W):\n",
    "    \"\"\"\n",
    "    Generate dense anchors given grid defined by (H,W)\n",
    "\n",
    "    Arguments:\n",
    "    anchors -- tensor of shape (num_anchors, 2), pre-defined anchors (pw, ph) on each cell\n",
    "    H -- int, grid height\n",
    "    W -- int, grid width\n",
    "\n",
    "    Returns:\n",
    "    all_anchors -- tensor of shape (H * W * num_anchors, 4) dense grid anchors (c_x, c_y, w, h)\n",
    "    \"\"\"\n",
    "\n",
    "    # number of anchors per cell\n",
    "    A = anchors.size(0)\n",
    "\n",
    "    # number of cells\n",
    "    K = H * W\n",
    "\n",
    "    shift_x, shift_y = torch.meshgrid([torch.arange(0, W), torch.arange(0, H)])\n",
    "\n",
    "    # transpose shift_x and shift_y because we want our anchors to be organized in H x W order\n",
    "    shift_x = shift_x.t().contiguous()\n",
    "    shift_y = shift_y.t().contiguous()\n",
    "\n",
    "    # shift_x is a long tensor, c_x is a float tensor\n",
    "    c_x = shift_x.float()\n",
    "    c_y = shift_y.float()\n",
    "\n",
    "    centers = torch.cat([c_x.view(-1, 1), c_y.view(-1, 1)], dim=-1)  # tensor of shape (h * w, 2), (cx, cy)\n",
    "\n",
    "    # add anchors width and height to centers\n",
    "    all_anchors = torch.cat([centers.view(K, 1, 2).expand(K, A, 2),\n",
    "                             anchors.view(1, A, 2).expand(K, A, 2)], dim=-1)\n",
    "\n",
    "    all_anchors = all_anchors.view(-1, 4)\n",
    "\n",
    "    return all_anchors\n",
    "\n",
    "def build_target(output, gt_data):\n",
    "    \"\"\"\n",
    "    Build the training target for output tensor\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    output_data -- tuple (delta_pred_batch, conf_pred_batch, class_pred_batch), output data of the yolo network\n",
    "    gt_data -- tuple (gt_boxes_batch, gt_classes_batch, num_boxes_batch), ground truth data\n",
    "\n",
    "    delta_pred_batch -- tensor of shape (B, H * W * num_anchors, 4), predictions of delta σ(t_x), σ(t_y), σ(t_w), σ(t_h)\n",
    "    conf_pred_batch -- tensor of shape (B, H * W * num_anchors, 1), prediction of IoU score σ(t_c)\n",
    "    class_score_batch -- tensor of shape (B, H * W * num_anchors, num_classes), prediction of class scores (cls1, cls2, ..)\n",
    "\n",
    "    gt_boxes_batch -- tensor of shape (B, N, 4), ground truth boxes, normalized values(x1, y1, x2, y2) range 0~1\n",
    "    gt_classes_batch -- tensor of shape (B, N), ground truth classes (cls)\n",
    "    num_obj_batch -- tensor of shape (B, 1). number of objects\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    iou_target -- tensor of shape (B, H * W * num_anchors, 1)\n",
    "    iou_mask -- tensor of shape (B, H * W * num_anchors, 1)\n",
    "    box_target -- tensor of shape (B, H * W * num_anchors, 4) \n",
    "    box_mask -- tensor of shape (B, H * W * num_anchors, 1)\n",
    "    class_target -- tensor of shape (B, H * W * num_anchors, 1)\n",
    "    class_mask -- tensor of shape (B, H * W * num_anchors, 1)\n",
    "\n",
    "    \"\"\"\n",
    "    bsize = output.size(0)\n",
    "    H = output.size(2)\n",
    "    W = output.size(3)\n",
    "\n",
    "    output = output.permute(0,2,3,1).reshape(-1,H,W,cfg.num_anchors,5+cfg.num_classes).reshape(-1,H*W*cfg.num_anchors,5+cfg.num_classes)\n",
    "    \n",
    "    xy_pred = torch.sigmoid(output[:,:,1:3])\n",
    "    hw_pred = torch.exp(output[:,:,3:5])\n",
    "    delta_pred_batch = torch.cat([xy_pred,hw_pred],dim=-1)\n",
    "    \n",
    "    conf_pred_batch = torch.sigmoid(output[:,:,0])\n",
    "    class_score_batch = torch.sigmoid(output[:,:,5:5+cfg.num_classes])\n",
    "\n",
    "    gt_boxes_batch = gt_data[0] / cfg.image_size\n",
    "    gt_classes_batch = gt_data[1]\n",
    "    num_boxes_batch = gt_data[2]\n",
    "\n",
    "    ### output base\n",
    "    iou_target = delta_pred_batch.new_zeros((bsize, H * W, cfg.num_anchors, 1))\n",
    "    iou_mask = delta_pred_batch.new_ones((bsize, H * W, cfg.num_anchors, 1)) * cfg.noobject_scale\n",
    "\n",
    "    box_target = delta_pred_batch.new_zeros((bsize, H * W, cfg.num_anchors, 4))\n",
    "    box_mask = delta_pred_batch.new_zeros((bsize, H * W, cfg.num_anchors, 1))\n",
    "\n",
    "    class_target = conf_pred_batch.new_zeros((bsize, H * W, cfg.num_anchors, 1))\n",
    "    class_mask = conf_pred_batch.new_zeros((bsize, H * W, cfg.num_anchors, 1))\n",
    "\n",
    "    # note: the all anchors' xywh scale is normalized by the grid width and height, i.e. 13 x 13\n",
    "    anchors = torch.FloatTensor(cfg.anchors)/32\n",
    "    all_grid_xywh = generate_all_anchors(anchors, H, W) # shape: (H * W * num_anchors, 4), format: (x, y, w, h)\n",
    "    all_grid_xywh = delta_pred_batch.new(*all_grid_xywh.size()).copy_(all_grid_xywh)\n",
    "    all_anchors_xywh = all_grid_xywh.clone()\n",
    "    all_anchors_xywh[:, 0:2] += 0.5\n",
    "    all_anchors_xxyy = xywh2xxyy(all_anchors_xywh)\n",
    "\n",
    "    ### 遍历batch中所有样本\n",
    "    for b in range(bsize):\n",
    "        delta_pred = delta_pred_batch[b]\n",
    "        num_obj = num_boxes_batch[b].item()\n",
    "        gt_boxes = gt_boxes_batch[b][:num_obj, :]\n",
    "        gt_classes = gt_classes_batch[b][:num_obj]\n",
    "\n",
    "        # rescale ground truth boxes\n",
    "        gt_boxes[:, 0::2] *= W \n",
    "        gt_boxes[:, 1::2] *= H\n",
    "\n",
    "        # step 1: process IoU target\n",
    "\n",
    "        # apply delta_pred to pre-defined anchors\n",
    "        all_anchors_xywh = all_anchors_xywh.view(-1, 4)\n",
    "        box_pred = box_transform_inv(all_grid_xywh, delta_pred) ### 生成最终的预测边框\n",
    "        box_pred = xywh2xxyy(box_pred)\n",
    "\n",
    "        # for each anchor, its iou target is corresponded to the max iou with any gt boxes\n",
    "        ious = box_ious(box_pred, gt_boxes) # shape: (H * W * num_anchors, num_obj)\n",
    "        ious = ious.view(-1, cfg.num_anchors, num_obj)\n",
    "        max_iou, _ = torch.max(ious, dim=-1, keepdim=True) # shape: (H * W, num_anchors, 1)\n",
    "\n",
    "        # iou_target[b] = max_iou\n",
    "\n",
    "        # we ignore the gradient of predicted boxes whose IoU with any gt box is greater than cfg.threshold\n",
    "        iou_thresh_filter = max_iou.view(-1) > cfg.thresh\n",
    "        n_pos = torch.nonzero(iou_thresh_filter).numel()\n",
    "\n",
    "        if n_pos > 0:\n",
    "            iou_mask[b][max_iou >= cfg.thresh] = 0\n",
    "\n",
    "        # step 2: process box target and class target\n",
    "        # calculate overlaps between anchors and gt boxes\n",
    "        overlaps = box_ious(all_anchors_xxyy, gt_boxes).view(-1, cfg.num_anchors, num_obj)\n",
    "        gt_boxes_xywh = xxyy2xywh(gt_boxes)\n",
    "\n",
    "        # iterate over all objects\n",
    "\n",
    "        for t in range(gt_boxes.size(0)):\n",
    "            # compute the center of each gt box to determine which cell it falls on\n",
    "            # assign it to a specific anchor by choosing max IoU\n",
    "\n",
    "            gt_box_xywh = gt_boxes_xywh[t]\n",
    "            gt_class = gt_classes[t]\n",
    "            cell_idx_x, cell_idx_y = torch.floor(gt_box_xywh[:2])\n",
    "            cell_idx = cell_idx_y * W + cell_idx_x\n",
    "            cell_idx = cell_idx.long()\n",
    "            \n",
    "            # update box_target, box_mask\n",
    "            overlaps_in_cell = overlaps[cell_idx, :, t]\n",
    "            argmax_anchor_idx = torch.argmax(overlaps_in_cell)\n",
    "\n",
    "            assigned_grid = all_grid_xywh.view(-1, cfg.num_anchors, 4)[cell_idx, argmax_anchor_idx, :].unsqueeze(0)\n",
    "            gt_box = gt_box_xywh.unsqueeze(0)\n",
    "            target_t = box_transform(assigned_grid, gt_box)\n",
    "            box_target[b, cell_idx, argmax_anchor_idx, :] = target_t.unsqueeze(0)\n",
    "            box_mask[b, cell_idx, argmax_anchor_idx, :] = 1\n",
    "\n",
    "            # update cls_target, cls_mask\n",
    "            class_target[b, cell_idx, argmax_anchor_idx, :] = gt_class\n",
    "            class_mask[b, cell_idx, argmax_anchor_idx, :] = 1\n",
    "\n",
    "            # update iou target and iou mask\n",
    "            iou_target[b, cell_idx, argmax_anchor_idx, :] = max_iou[cell_idx, argmax_anchor_idx, :]\n",
    "            iou_mask[b, cell_idx, argmax_anchor_idx, :] = cfg.object_scale\n",
    "\n",
    "    return iou_target.view(bsize, -1, 1), \\\n",
    "           iou_mask.view(bsize, -1, 1), \\\n",
    "           box_target.view(bsize, -1, 4),\\\n",
    "           box_mask.view(bsize, -1, 1), \\\n",
    "           class_target.view(bsize, -1, 1).long(), \\\n",
    "           class_mask.view(bsize, -1, 1)\n",
    "\n",
    "\n",
    "def yolo_loss(output, target):\n",
    "    \"\"\"\n",
    "    Build yolo loss\n",
    "\n",
    "    Arguments:\n",
    "    output -- tuple (delta_pred, conf_pred, class_score), output data of the yolo network\n",
    "    target -- tuple (iou_target, iou_mask, box_target, box_mask, class_target, class_mask) target label data\n",
    "\n",
    "    delta_pred -- Variable of shape (B, H * W * num_anchors, 4), predictions of delta σ(t_x), σ(t_y), σ(t_w), σ(t_h)\n",
    "    conf_pred -- Variable of shape (B, H * W * num_anchors, 1), prediction of IoU score σ(t_c)\n",
    "    class_score -- Variable of shape (B, H * W * num_anchors, num_classes), prediction of class scores (cls1, cls2 ..)\n",
    "\n",
    "    iou_target -- Variable of shape (B, H * W * num_anchors, 1)\n",
    "    iou_mask -- Variable of shape (B, H * W * num_anchors, 1)\n",
    "    box_target -- Variable of shape (B, H * W * num_anchors, 4)\n",
    "    box_mask -- Variable of shape (B, H * W * num_anchors, 1)\n",
    "    class_target -- Variable of shape (B, H * W * num_anchors, 1)\n",
    "    class_mask -- Variable of shape (B, H * W * num_anchors, 1)\n",
    "\n",
    "    Return:\n",
    "    loss -- yolo overall multi-task loss\n",
    "    \"\"\"\n",
    "    bsize = output.size(0)\n",
    "    H = output.size(2)\n",
    "    W = output.size(3)\n",
    "\n",
    "    output = output.permute(0,2,3,1).reshape(-1,H,W,cfg.num_anchors,5+cfg.num_classes).reshape(-1,H*W*cfg.num_anchors,5+cfg.num_classes)\n",
    "    \n",
    "    xy_pred = torch.sigmoid(output[:,:,1:3])\n",
    "    hw_pred = torch.exp(output[:,:,3:5])\n",
    "    delta_pred_batch = torch.cat([xy_pred,hw_pred],dim=-1)\n",
    "    \n",
    "    conf_pred_batch = torch.sigmoid(output[:,:,0]).unsqueeze(-1)\n",
    "    class_score_batch = torch.sigmoid(output[:,:,5:5+cfg.num_classes])\n",
    "\n",
    "    iou_target = target[0]\n",
    "    iou_mask = target[1]\n",
    "    box_target = target[2]\n",
    "    box_mask = target[3]\n",
    "    class_target = target[4]\n",
    "    class_mask = target[5]\n",
    "\n",
    "    b, _, num_classes = class_score_batch.size()\n",
    "    class_score_batch = class_score_batch.view(-1, num_classes)\n",
    "    class_target = class_target.view(-1)\n",
    "    class_mask = class_mask.view(-1)\n",
    "\n",
    "    # ignore the gradient of noobject's target\n",
    "    class_keep = class_mask.nonzero().squeeze(1)\n",
    "    class_score_batch_keep = class_score_batch[class_keep, :]\n",
    "    class_target_keep = class_target[class_keep]\n",
    "    \n",
    "    # calculate the loss, normalized by batch size.\n",
    "    box_loss = 1 / b * cfg.coord_scale * F.mse_loss(delta_pred_batch * box_mask, box_target * box_mask, reduction='sum') / 2.0\n",
    "    iou_loss = 1 / b * F.mse_loss(conf_pred_batch * iou_mask, iou_target * iou_mask, reduction='sum') / 2.0\n",
    "    class_loss = 1 / b * cfg.class_scale * F.cross_entropy(class_score_batch_keep, class_target_keep, reduction='sum')\n",
    "\n",
    "    return box_loss, iou_loss, class_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0007d2ec-cf2f-4852-b9fd-b3e559805f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_iou(box1, box2, x1y1x2y2=True, GIoU=False, DIoU=False, CIoU=False, eps=1e-9):\n",
    "    # Returns the IoU of box1 to box2. box1 is 4, box2 is nx4\n",
    "    box2 = box2.T\n",
    " \n",
    "    # Get the coordinates of bounding boxes\n",
    "    if x1y1x2y2:  # x1, y1, x2, y2 = box1\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1[0], box1[1], box1[2], box1[3]\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2[0], box2[1], box2[2], box2[3]\n",
    "    else:  # transform from xywh to xyxy\n",
    "        b1_x1, b1_x2 = box1[0] - box1[2] / 2, box1[0] + box1[2] / 2\n",
    "        b1_y1, b1_y2 = box1[1] - box1[3] / 2, box1[1] + box1[3] / 2\n",
    "        b2_x1, b2_x2 = box2[0] - box2[2] / 2, box2[0] + box2[2] / 2\n",
    "        b2_y1, b2_y2 = box2[1] - box2[3] / 2, box2[1] + box2[3] / 2\n",
    " \n",
    "    # Intersection area\n",
    "    inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n",
    "            (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n",
    " \n",
    "    # Union Area\n",
    "    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n",
    "    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n",
    "    union = w1 * h1 + w2 * h2 - inter + eps\n",
    " \n",
    "    iou = inter / union\n",
    "    if GIoU or DIoU or CIoU:\n",
    "        cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)  # convex (smallest enclosing box) width\n",
    "        ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)  # convex height\n",
    "        if CIoU or DIoU:  # Distance or Complete IoU https://arxiv.org/abs/1911.08287v1\n",
    "            c2 = cw ** 2 + ch ** 2 + eps  # convex diagonal squared\n",
    "            rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 +\n",
    "                    (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4  # center distance squared\n",
    "            if DIoU:\n",
    "                return iou - rho2 / c2  # DIoU\n",
    "            elif CIoU:  # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n",
    "                v = (4 / math.pi ** 2) * torch.pow(torch.atan(w2 / h2) - torch.atan(w1 / h1), 2)\n",
    "                with torch.no_grad():\n",
    "                    alpha = v / ((1 + eps) - iou + v)\n",
    "                return iou - (rho2 / c2 + v * alpha)  # CIoU\n",
    "        else:  # GIoU https://arxiv.org/pdf/1902.09630.pdf\n",
    "            c_area = cw * ch + eps  # convex area\n",
    "            return iou - (c_area - union) / c_area  # GIoU\n",
    "    else:\n",
    "        return iou  # IoU\n",
    "\n",
    "def Self_NMS(boxes, scores, iou_thres, GIoU=False, DIoU=False, CIoU=False):\n",
    "    \"\"\"\n",
    "    :param boxes:  (Tensor[N, 4])): are expected to be in ``(x1, y1, x2, y2)\n",
    "    :param scores: (Tensor[N]): scores for each one of the boxes\n",
    "    :param iou_thres: discards all overlapping boxes with IoU > iou_threshold\n",
    "    :return:keep (Tensor): int64 tensor with the indices\n",
    "            of the elements that have been kept\n",
    "            by NMS, sorted in decreasing order of scores\n",
    "    \"\"\"\n",
    "    # 按conf从大到小排序\n",
    "    B = torch.argsort(scores, dim=-1, descending=True) ##返回的是索引值\n",
    "    keep = []\n",
    "    while B.numel() > 0:\n",
    "        # 取出置信度最高的\n",
    "        index = B[0]\n",
    "        keep.append(index)\n",
    "        if B.numel() == 1: break\n",
    "        # 计算iou,根据需求可选择GIOU,DIOU,CIOU\n",
    "        iou = bbox_iou(boxes[index, :], boxes[B[1:], :], GIoU=GIoU, DIoU=DIoU, CIoU=CIoU)\n",
    "        # 找到符合阈值的下标\n",
    "        inds = torch.nonzero(iou <= iou_thres).reshape(-1)\n",
    "        ##这里主要是处理一个索引的问题，这里计算iou的时候是用score最高的box与其他box计算，得到的iou是个列表\n",
    "        ##但此时的iou列表已经比B少了一个值了，ins返回的是iou < 阈值的框在iou列表里面的索引\n",
    "        ##这时要返回其在B中的索引就需要把inds+1，然后得到进行一轮NMS后剩下的框\n",
    "        B = B[inds + 1]\n",
    "    return torch.tensor(keep)\n",
    "\n",
    "def Full_NMS(output,thresh=cfg.thresh):\n",
    "    B = output.size(0)\n",
    "    H = output.size(-1)\n",
    "    W = output.size(-2)\n",
    "    grid_size = cfg.image_size / H\n",
    "    output = output.permute(0,2,3,1).reshape(-1,H,W,cfg.num_anchors,5+cfg.num_classes).reshape(-1,H*W*cfg.num_anchors,5+cfg.num_classes)\n",
    "    output[:,:,:3] = torch.sigmoid(output[:,:,:3])\n",
    "    output[:,:,3:5] = torch.exp(output[:,:,3:5])\n",
    "    output[:,:,5:] = torch.softmax(output[:,:,5:],dim=-1)\n",
    "    delta_pred_batch = output[:,:,1:5]\n",
    "    all_grid_xywh = generate_all_anchors(torch.FloatTensor(cfg.anchors)/grid_size, H, W)\n",
    "    all_grid_xywh = delta_pred_batch.new(*all_grid_xywh.size()).copy_(all_grid_xywh)\n",
    "    all_anchors_xywh = all_grid_xywh.clone()\n",
    "    all_anchors_xywh[:, 0:2] += 0.5\n",
    "    result = []\n",
    "    for b in range (B):\n",
    "        box_pred = box_transform_inv(all_grid_xywh, output[b,:,1:5])\n",
    "        box_pred = xywh2xxyy(box_pred)\n",
    "        batch_result = {}\n",
    "        for c in range (cfg.num_classes):\n",
    "            probs = output[b,:,0] * output[b,:,5+c]\n",
    "            keep  = Self_NMS(box_pred,probs,thresh)\n",
    "            print(keep.shape,keep)\n",
    "            batch_result[c] = box_pred[keep] * grid_size\n",
    "        result.append(batch_result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3e39c6-3cb4-4647-b98c-43c0f53d1961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, test_loader, epochs):\n",
    "\n",
    "    model.cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr = cfg.lr)\n",
    "\n",
    "    for epoch in range (epochs):\n",
    "        model.train()\n",
    "        t_box_loss, t_iou_loss, t_class_loss = 0, 0, 0\n",
    "        for images, groundTruth in tqdm(data_loader):\n",
    "            images = images.cuda()\n",
    "            groundTruth = {k:v.cuda() for k,v in groundTruth.items()}\n",
    "    \n",
    "            output = model(images)\n",
    "    \n",
    "            targets = build_target(output,(groundTruth[\"boxes\"],groundTruth[\"labels\"],groundTruth[\"num_objs\"]))\n",
    "    \n",
    "            box_loss, iou_loss, class_loss = yolo_loss(output,targets)\n",
    "            t_box_loss += box_loss.item()\n",
    "            t_iou_loss += iou_loss.item()\n",
    "            t_class_loss += class_loss.item()\n",
    "            loss = box_loss + iou_loss + class_loss\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print(box_loss, iou_loss, class_loss)\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        batch = 0\n",
    "        for images, groundTruth in tqdm(test_loader):\n",
    "            images = images.cuda()\n",
    "            groundTruth = {k:v.cuda() for k,v in groundTruth.items()}\n",
    "            output = model(images)\n",
    "            result = Full_NMS(output)\n",
    "            tp = 0\n",
    "            fp = 0\n",
    "            t = 0\n",
    "            for b in range (images.size(0)):\n",
    "                image = images[b]\n",
    "                num_objs = groundTruth[\"num_objs\"][b]\n",
    "                boxes = groundTruth[\"boxes\"][b][:num_objs,:]\n",
    "                labels = groundTruth[\"labels\"][b][:num_objs]\n",
    "                t += num_objs\n",
    "                for c in range (cfg.num_classes):\n",
    "                    label_boxes = boxes[torch.where(labels==c)]\n",
    "                    pred_boxes = result[b][c]\n",
    "                    if len (label_boxes) == 0:\n",
    "                        fp += len(pred_boxes)\n",
    "                        continue\n",
    "                    ious,_ = torch.max(box_ious(pred_boxes,label_boxes),dim=-1)\n",
    "                    tp += torch.sum(ious>0.95).item()\n",
    "                    fp += pred_boxes.size(0) - tp\n",
    "            precision += tp / (tp+fp)\n",
    "            recall += tp / t\n",
    "            batch += 1\n",
    "        precision /= batch\n",
    "        recall /= batch\n",
    "        print(precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e91f08-5887-42b0-a3f2-944c49521b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YoloV2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd68dea-3646-4628-865d-16f90ce62d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_data_loader, val_data_set_loader, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06c2de7-48fa-43c2-aee4-892bc71aa902",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x, y = val_data_set.__getitem__(5)\n",
    "    output = model(x.unsqueeze(0).cuda())\n",
    "    result = Full_NMS(output.detach().cpu(),thresh=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf83f23c-5a0e-4d80-b7c5-a162eacafab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in result[0].items():\n",
    "    print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a801e39f-1554-4994-a654-4d9949602f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(x.detach().cpu().numpy().transpose(1, 2, 0)) \n",
    "\n",
    "for box in result[0][0].numpy():\n",
    "    x1, y1, x2, y2 = box\n",
    "    plt.plot([x1, x2, x2, x1, x1], [y1, y1, y2, y2, y1], color='red')  # 绘制矩形框\n",
    "\n",
    "plt.title(\"图片和预测框展示\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b9aa60-a591-45b3-9e34-18ac437dde04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f346e1e7-1169-411f-8e8c-12742f691aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5992d683-ac7a-4532-bf5e-4a839963a0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
